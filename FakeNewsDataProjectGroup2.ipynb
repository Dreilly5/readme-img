{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dreilly5/readme-img/blob/main/FakeNewsDataProjectGroup2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bVsQpweMoas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7431863-16c4-46dd-878c-f8a08ce244d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import warnings\n",
        "import nltk\n",
        "from google.colab import drive\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS IS FOR THIERRY's Implementation only\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGmAMUsdKn1f",
        "outputId": "e569bb6a-02fd-45c5-ee14-432970bfac2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Store The files as variables"
      ],
      "metadata": {
        "id": "-rF0F3--cx6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "test_labels_raw = pd.read_csv('submit.csv')\n"
      ],
      "metadata": {
        "id": "fPF-TTS_PIzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is for Thierry's implementation only\n",
        "training = '/content/drive/MyDrive/FakeNewsData/train.csv'\n",
        "testing = '/content/drive/MyDrive/FakeNewsData/test.csv'\n",
        "train_data = pd.read_csv(training)\n",
        "test_data = pd.read_csv(testing)\n",
        "print(os.listdir('/content/drive/MyDrive/FakeNewsData/'))\n",
        "test_labels_raw = pd.read_csv('/content/drive/MyDrive/FakeNewsData/submit.csv')"
      ],
      "metadata": {
        "id": "8xsUusQmKse8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop The Empty Values"
      ],
      "metadata": {
        "id": "ZeR8ZSCTnl2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "id": "poI_6TmUm-Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: drop the train_data and test_data with null text data\n",
        "\n",
        "train_data.dropna(subset=['text'], inplace=True)\n",
        "test_data.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "print (train_data.info())\n"
      ],
      "metadata": {
        "id": "T57zPiMsn40J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Add Column for Final news with title and text"
      ],
      "metadata": {
        "id": "FfxlHo2Vdptu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (train_data.shape)\n",
        "print (test_data.shape)\n",
        "\n",
        "train_data['news'] = train_data['title'] + ' ' + train_data['text']\n",
        "test_data['news'] = test_data['title'] + ' ' + test_data['text']\n",
        "\n",
        "print (train_data.shape)\n",
        "print (test_data.shape)\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "YDDtstj2WXym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drop useless columns"
      ],
      "metadata": {
        "id": "LmQxWpsCpGs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delete_columns = ['title', 'text', 'author']\n",
        "train_data = train_data.drop(delete_columns, axis=1)\n",
        "test_data = test_data.drop(delete_columns, axis=1)\n",
        "train_data.head()\n"
      ],
      "metadata": {
        "id": "cTf4y4K3bvU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Next we need to filter out the (special chars and extra space and newlines)"
      ],
      "metadata": {
        "id": "eXfg5BkzmdHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['news'] = train_data['news'].str.lower()\n",
        "test_data['news'] = test_data['news'].str.lower()\n",
        "train_data['news'] = train_data['news'].str.replace(\"[^a-zA-Z0-9\\s]\", \"\")\n",
        "train_data['news'] = train_data['news'].str.replace(\"\\n\", \"\")\n",
        "train_data['news'] = train_data['news'].str.replace('\\s+','')\n",
        "test_data['news'] = test_data['news'].str.replace(\"[^a-zA-Z0-9/s]\", \"\")\n",
        "test_data['news'] = test_data['news'].str.replace(\"\\n\", \"\")\n",
        "test_data['news'] = test_data['news'].str.replace(\"\\s+\", \"\")\n",
        "train_data.head()\n",
        "test_data.head()"
      ],
      "metadata": {
        "id": "ADI5mcDpmllv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print out train data news index 4\n",
        "\n",
        "print(train_data['news'][4])\n"
      ],
      "metadata": {
        "id": "rSlnuYQyrN9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Remove Float Types"
      ],
      "metadata": {
        "id": "5DwPayHUPJyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['news'] = train_data['news'].astype(str)\n",
        "test_data['news'] = test_data['news'].astype(str)\n"
      ],
      "metadata": {
        "id": "q2_193_gPR3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Removing Stopwords"
      ],
      "metadata": {
        "id": "3LnnUnRPLN7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: remove stopwords\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "train_data['news'] = train_data['news'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "test_data['news'] = test_data['news'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
      ],
      "metadata": {
        "id": "SpPdh8hPCkY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Turning Data Into Tensors"
      ],
      "metadata": {
        "id": "WeCa--PwjOza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_data))\n",
        "print(type(train_data['news']))\n",
        "batchsize = 4"
      ],
      "metadata": {
        "id": "QQIKhMoIj0eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing training data\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, padded_sequences, labels):\n",
        "        self.padded_sequences = padded_sequences\n",
        "        if isinstance(labels, torch.Tensor):\n",
        "            self.labels = labels.unsqueeze(1) if len(labels.shape) == 1 else labels\n",
        "        else:\n",
        "            self.labels = torch.tensor(labels.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.padded_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.padded_sequences[idx], self.labels[idx]\n",
        "\n",
        "# Tokenizing training data\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "train_tokens = train_data['news'].apply(tokenizer)\n",
        "\n",
        "# Build vocabulary\n",
        "train_vocab = build_vocab_from_iterator(train_tokens, specials=[\"<unk>\"])\n",
        "train_vocab.set_default_index(train_vocab[\"<unk>\"])\n",
        "\n",
        "def numerical_encoding(token_list):\n",
        "    return train_vocab(token_list)\n",
        "\n",
        "train_sequences = train_tokens.apply(numerical_encoding)\n",
        "\n",
        "# Pad sequences\n",
        "train_padded_sequences = torch.nn.utils.rnn.pad_sequence(\n",
        "    [torch.tensor(seq) for seq in train_sequences], batch_first=True\n",
        ")\n",
        "\n",
        "# Convert labels to tensor\n",
        "train_labels = torch.tensor(train_data[\"label\"].values, dtype=torch.float32)  # Ensure dtype matches\n",
        "\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = NewsDataset(train_padded_sequences, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "id": "zcPz0-KbS23T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_labels))"
      ],
      "metadata": {
        "id": "qBAY5FmHcYKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing testing data\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "test_tokens = test_data['news'].apply(tokenizer)\n",
        "\n",
        "# Build vocabulary (ensure it matches with training vocabulary or use the same vocab)\n",
        "test_vocab = build_vocab_from_iterator(test_tokens, specials=[\"<unk>\"])\n",
        "test_vocab.set_default_index(test_vocab[\"<unk>\"])\n",
        "\n",
        "test_sequences = test_tokens.apply(numerical_encoding)\n",
        "\n",
        "# Pad sequences\n",
        "test_padded_sequences = torch.nn.utils.rnn.pad_sequence(\n",
        "    [torch.tensor(seq) for seq in test_sequences], batch_first=True\n",
        ")\n",
        "\n",
        "# Convert labels to tensor\n",
        "test_labeled = test_labels_raw['label'].astype(int)\n",
        "labels = torch.tensor(test_labeled.values, dtype=torch.float32)  # Ensure dtype matches\n",
        "test_labels = labels.unsqueeze(1)\n",
        "\n",
        "# Prepare test dataset and dataloader\n",
        "test_labels = test_labels_raw['label']\n",
        "test_dataset = NewsDataset(test_padded_sequences, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "RIpf05axmWUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_padded_sequences[1].size())\n",
        "print(train_padded_sequences[1].size())\n",
        "print(type(train_padded_sequences))"
      ],
      "metadata": {
        "id": "hlovlFxYmkdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desired_length = train_padded_sequences[1].size(0)\n",
        "\n",
        "# Function to pad a single sequence tensor to the desired length\n",
        "def pad_sequence(seq_tensor, desired_length, padding_value):\n",
        "    length = seq_tensor.size(0)\n",
        "    if length < desired_length:\n",
        "        padding = (0, desired_length - length)\n",
        "        return torch.nn.functional.pad(seq_tensor, padding, value=padding_value)\n",
        "    else:\n",
        "        return seq_tensor[:desired_length]\n",
        "\n",
        "test_padded_sequences = torch.stack([pad_sequence(seq, desired_length, test_vocab[\"<unk>\"]) for seq in test_padded_sequences])\n"
      ],
      "metadata": {
        "id": "P6qyJyeVoY78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_padded_sequences.shape)\n",
        "print(train_padded_sequences.shape)"
      ],
      "metadata": {
        "id": "P_YuQfbBohdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building A Basic RNN"
      ],
      "metadata": {
        "id": "p8X4tJBBxy1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to build RNN model in general\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Define the embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Define the RNN layer\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim,  # Use embedding_dim as input size\n",
        "                          hidden_size=hidden_size,\n",
        "                          num_layers=num_layers,\n",
        "                          batch_first=True,\n",
        "                          nonlinearity='relu')  # Use ReLU activation function\n",
        "\n",
        "        # Define the output layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply embedding layer\n",
        "        x = self.embedding(x)  # Convert token indices to embeddings\n",
        "\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Pass through RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # Pass through output layer\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "xNSTpRSAx5sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declaring first preliminary model\n",
        "input_size = 64\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "num_layers = 6\n",
        "\n",
        "first_model = RNNModel(len(train_vocab), input_size, hidden_size, output_size, num_layers)"
      ],
      "metadata": {
        "id": "1Mu_MLeNlmkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running The Initial RNN"
      ],
      "metadata": {
        "id": "qJMUQszQHX1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_loader))\n",
        "print(type(test_loader))\n",
        "for inputs, labels in train_loader:\n",
        "    print(f'Batch input shape: {inputs.shape}')\n",
        "    print(f'Batch label shape: {labels.shape}')\n",
        "    break"
      ],
      "metadata": {
        "id": "aygWVWBMLEUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = optim.Adam(first_model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "first_train_acc = []\n",
        "first_train_loss = []\n",
        "first_val_acc = []\n",
        "first_val_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    first_model.train()\n",
        "    train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = first_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / total_train\n",
        "    train_accuracy = correct_train / total_train\n",
        "\n",
        "    first_train_loss.append(train_loss)\n",
        "    first_train_acc.append(train_accuracy)\n",
        "\n",
        "    # Validation phase\n",
        "    first_model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            print(inputs)\n",
        "            outputs = first_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = val_loss / total_val\n",
        "    val_accuracy = correct_val / total_val\n",
        "\n",
        "    first_val_loss.append(val_loss)\n",
        "    first_val_acc.append(val_accuracy)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')"
      ],
      "metadata": {
        "id": "bO09PPX6HauY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}